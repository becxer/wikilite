
반달이 앞으로 해야할 일?
-------

1. Brain 을 Small brain 과 Big brain으로 나누어서 처리한다.
2. Chat mode 와 Doit mode 로 나누도록한다. 
3. Speech 모듈을 다른것을 찾아본다.
4. 라즈베리파이에 올려본다.

업그레이드 된 반달이 프로젝트 설계(stver.2)
--------

1. waken 모듈이 silence를 기반으로 pure 음성인식을 수행한다.

	이안에서 여러가지 모듈이 나올수있다. 예를들면,
	* 말하는것 인식하기
	* 화자가 누군지 인식하기

2. ear 모듈이 speech to text를 통해 pcm->txt 를 수행한다.

	이것도 선택지가 여러개 있다.
	* 구글 speech-api
	* 다음 speech-api
	* kaldi 오픈소스로 내가 만들기
	* htk 오픈소스로 내가 만들기

3. brain 모듈이 txt를 가지고 어떤 명령을 수행할 것인지 정하고, 수행하는 control tower 역할을 수행한다.
	
	이안에서 나중에 모듈들이 많이 파생될 수 있을것 같다.
	* 대화 모듈
	* 상황에 따른 대화 모듈
	* 명령어 모듈
	
	따라서, 지금은 잘 모르겠지만 나중에 small-brain , big-brain  으로 나누어서
	전처리 과정을 small-brain 으로 놓아야 겠다.
	
4. mouth 모듈이 brain의 최종 응답을 음성으로 바꾸어 준다.

	이것도 선택지가 여러개 있다.
	* 구글 translater url 훔쳐다 쓰기
	* 오픈소스 쓰기( 생각보다 구하기 쉬움 )
	
	

반달이 프로젝트 설계(stver.1)
-------

뭐부터 해야할까 음성인식 프로세스는 다음과 같이 나눌수있겠지.

<가장 간단한 명령-수행 버전>

1. 마이크로 음성을 듣는다.(silence gap을 파악하는것도 필요하겠지)
2. 음성을 음성인식기로 보낸다.
3. 음성인식 결과 text를 형태소별로 자른다. 혹은 단어별.
4. 명령을 인식한다. 수행한다.



